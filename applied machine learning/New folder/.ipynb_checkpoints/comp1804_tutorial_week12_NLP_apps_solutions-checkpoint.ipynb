{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-cdrUuZQdm-"
   },
   "source": [
    "# Natural Language Processing Applications\n",
    "\n",
    "Author: Stef Garasto. Released under the [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) licence.\n",
    "\n",
    "The second part is a slightly modified version of this [HuggingFace tutorial](https://huggingface.co/course/chapter7/7?fw=pt#the-squad-dataset) (also released under the [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) licence).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pV8AK8DQZmE"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bdlh3LP2rKmm"
   },
   "source": [
    "This notebook contains the tutorial for the class on natural language processing with deep learning as part of the module \"Artificial Intelligence Applications\".\n",
    "\n",
    "There are two goals for this tutorial:\n",
    "\n",
    "1. Learn how to use BERT (or similar) embeddings to find similar words.\n",
    "2. Fine-tune a question-answering system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShxkhsourLDR"
   },
   "source": [
    "## Prerequisites\n",
    "To execute the code, click on the corresponding cell and press SHIFT + ENTER or the little \"play\" button on the left.\n",
    "\n",
    "You can set up this notebook to run on a CPU (the default) or on a GPU. To change from GPU to CPU go to the menu bar above and click 'Runtime --> Change runtime type'. Then select 'GPU' from the dropdown box under 'Hardware accelerator' (switch back to 'None' to use a CPU).\n",
    "\n",
    "You can also download this notebook to run it locally. However, note that (on top of Python 3+) these are the prerequisites:\n",
    "\n",
    "Packages needed:\n",
    "*   Scikit-learn\n",
    "*   Jupyter notebook\n",
    "*   Pandas\n",
    "*   Numpy\n",
    "*   Matplotlib\n",
    "*   Pytorch\n",
    "*   transformers\n",
    "*   wget\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxJl6CR2rnHK"
   },
   "source": [
    "## Datasets\n",
    "\n",
    "We will use two main datasets in this tutorial:\n",
    "\n",
    "1. [The ESCO classification](http://ec.europa.eu/esco) of the European Commission.\n",
    "2. The [Stanford Question Answering Dataset (SQuAD)](https://rajpurkar.github.io/SQuAD-explorer/) dataset.\n",
    "\n",
    "More details on each dataset can be found in the sections below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_A9RKP3r4KG"
   },
   "source": [
    "# Setting things up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BMLvNbmB_BpK"
   },
   "outputs": [],
   "source": [
    "# variable to define whether the notebook is being run locally rather than on Colab\n",
    "# by default, the notebook is run on Colab - change the flag to true if you're running locally.\n",
    "LOCALRUN = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9RDuWcxx0UY"
   },
   "outputs": [],
   "source": [
    "# install some packages that we'll need that are not accessible by default in Colab\n",
    "if LOCALRUN:\n",
    "    print(\"Make sure the packages 'transformers' and 'wget' have been installed with 'pip install'\")\n",
    "else:\n",
    "    !pip install transformers[sentencepiece]\n",
    "    !pip install accelerate\n",
    "    !apt install git-lfs\n",
    "    print('---> Installed transformers')\n",
    "    !pip install wget\n",
    "    print('---> Installed wget')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPHnr9mzr6-d"
   },
   "outputs": [],
   "source": [
    "# define imports\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import wget\n",
    "import collections\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "if not LOCALRUN:\n",
    "    % matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSm3TJ8bjjm_"
   },
   "source": [
    "SET UP PYTORCH USE\n",
    "\n",
    "[Pytorch](pytorch.org/) is a deep learning library and an alternative to Keras. \n",
    "\n",
    "With pytorch, you can specify explicitely whether to train on GPU or on CPU by defining a \"device\" variable. Then you can \"move\" a model and a dataset to that device memory so that it can be accessed more efficiently. You move a model to a device by calling `model.to(device)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IRnp8OpE43gy"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define where we want to train the model\n",
    "# If there's a GPU available...\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "if USE_GPU:    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ilN014nZYcA"
   },
   "outputs": [],
   "source": [
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "etDxvtBAPq7c"
   },
   "outputs": [],
   "source": [
    "# Just FYI, if you need to clone git hub repos from within google colab\n",
    "#!git clone <name-of the repo>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqtqXlRR9wuM"
   },
   "source": [
    "## Transformer library imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ihOywDAB9xq8"
   },
   "outputs": [],
   "source": [
    "# huggingface's transformers import\n",
    "from transformers import AutoTokenizer, AutoModel # we've seen these in week 10 \n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7IC_lo_r17K"
   },
   "source": [
    "# Finding synonyms using embeddings\n",
    "\n",
    "Here, we will turn each word in the text we are working with into a word embedding using the pre-trained BERT. We can then use the embeddings directly for any task we want to do. Using the embeddings this way corresponds to using BERT (or similar models) with all the layers \"frozen\" (feature-extraction), as we saw in week 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BLcJF8Jcfhq"
   },
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQ64kj2ZchsG"
   },
   "source": [
    "We're going to use the [ESCO dataset](https://ec.europa.eu/esco/), a dataset on skills used in the labour market. ESCO is the multilingual classification of European Skills, Competences, Qualifications and Occupations. It was created by the European Commission.\n",
    "\n",
    "The ESCO classification identifies and categorises skills, competences, qualifications and occupations relevant for the EU labour market and education and training. It systematically shows the relationships between the different concepts.\n",
    "\n",
    "The part of interest to us is the list of thousands of labour market skills (together with their descriptions, if you're interested).\n",
    "\n",
    "I've left a copy of it accessible from my Google Drive, but it won't stay there forever after the end of the module, so I'd suggest downloading a local copy from Moodle!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sMcULWCIZVUn"
   },
   "outputs": [],
   "source": [
    "# ESCO datasets\n",
    "\n",
    "print('Downloading dataset...')\n",
    "\n",
    "# Download the file using the URL for the dataset zip file. \n",
    "# Replace with your own google file ID if you have stored a version in your own google drive\n",
    "# The ID is everything that comes after 'id='\n",
    "if LOCALRUN:\n",
    "    esco_url = 'https://docs.google.com/uc?export=download&id=1qu6_Hmed2mtSt9gLssLL3CjMh7ruXd4_'\n",
    "    if not os.path.exists('./esco_public_1.1.zip/'):\n",
    "        wget.download(esco_url, './esco_public_1.1.zip')\n",
    "else:\n",
    "    !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1qu6_Hmed2mtSt9gLssLL3CjMh7ruXd4_' -O esco_public_1.1.zip\n",
    "    \n",
    "if LOCALRUN:\n",
    "    print(('The dataset has been downloaded in a file called esco_public_1.1.zip within '\n",
    "          'the folder where this script lives.'))\n",
    "    print(\"Please go to that folder and unzip it in the same folder (not in a subfolder). \")\n",
    "    print(\"The process itself should create a new subfolder called 'v1.0.8'.\")\n",
    "else:\n",
    "    # Unzip the dataset (if we haven't already)\n",
    "    if not os.path.exists('./v1.0.8/'):\n",
    "        !unzip esco_public_1.1.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iBwZYaKhaP8A"
   },
   "outputs": [],
   "source": [
    "# Load the dataset into a pandas dataframe.\n",
    "esco_df = pd.read_csv(\"./v1.0.8/skills_en.csv\", usecols = ['preferredLabel','description'])\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of skills in the ESCO dataset is: {:,}\\n'.format(esco_df.shape[0]))\n",
    "\n",
    "# Display 10 random rows from the data.\n",
    "esco_df.sample(10, random_state = seed_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ABCZQRHmi5k"
   },
   "source": [
    "There are two columns: 'preferredLabel' with the name of the skill and 'description' which an explanation for the skill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dINSbQXMl5t"
   },
   "source": [
    "## Load pre-trained BERT\n",
    "Load both the tokenizer and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zwtQDjL6SFy_"
   },
   "outputs": [],
   "source": [
    "# transformers imports\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iR12l674MlQW"
   },
   "outputs": [],
   "source": [
    "# load tokenizer\n",
    "feat_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', # we use the uncased BERT Base model (12 encoders)\n",
    "                                          do_lower_case = True #everything lower case\n",
    "                                          )\n",
    "\n",
    "# load BERT model\n",
    "bert_model = AutoModel.from_pretrained('bert-base-uncased',\n",
    "                                       output_hidden_states=True # tell the model we want all the hidden states\n",
    "                                       )\n",
    "\n",
    "#set in evaluation model\n",
    "bert_model.eval()\n",
    "\n",
    "# move model to GPU if needed\n",
    "if USE_GPU:\n",
    "    bert_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n7j6aBCNm1Wn"
   },
   "outputs": [],
   "source": [
    "# show example skill\n",
    "text_skill = esco_df.iloc[1000].preferredLabel\n",
    "text_skill\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uu-fHgP7r2p7"
   },
   "outputs": [],
   "source": [
    "# Let's try once to pass the test skill through the BERT model, what do we get?\n",
    "# First, tokenize to put the input in a format that BERT understands\n",
    "feat_encoded_dict = feat_tokenizer(text_skill,\n",
    "                         add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                         return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                         padding = True\n",
    "                         )\n",
    "\n",
    "token_ids = feat_encoded_dict['input_ids']\n",
    "print(f'The number of tokesn (including 2 special ones is: {token_ids.shape[1]})')\n",
    "\n",
    "# then, pass the input through the model\n",
    "with torch.no_grad(): # no_grad because we don't want to train BERT\n",
    "    bert_output = bert_model(token_ids.to(device), \n",
    "                           token_type_ids = None #no need for this since it's a single sentence\n",
    "                           )\n",
    "\n",
    "print(f'There are {len(bert_output)} items in the output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJpxHoW-o_XV"
   },
   "source": [
    "What's BERT's output made of?\n",
    "\n",
    "Well, the exact output will depend on the parameters we used when defining the model with 'from_pretrained'. In this case, we used output_hidden_states= True and output_attention = False (the default), so the third item of the output is the list of all the hidden states from all the encoding layers of BERT. (The first item is the output of the last layer and the second item is the output of the [CLS] token).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QR32UPfQw3aw"
   },
   "outputs": [],
   "source": [
    "# Let's explore the full hidden states output\n",
    "hidden_states = bert_output[2]\n",
    "print(f'The full hidden states output is a {type(hidden_states)}')\n",
    "print(f'It is a list of {len(hidden_states)}, each with {len(hidden_states[0].shape)} dimensions (size: {hidden_states[0].shape})')\n",
    "\n",
    "# The list has 13 elements because BERT Base has 13 layers (1 embedding + 12 encoders)\n",
    "# Each element of the list is the full output of that layer with dimensions: #samples x #tokens x #features (768)\n",
    "# For us, #samples = 1 always because we'll apply BERT to one sentence at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJzaKt3WrYDT"
   },
   "source": [
    "So, basically we have 13 different embeddings for each token, and 13 x #tokens different embeddings for the whole sentece. How do we combine them?\n",
    "\n",
    "To obtain one embedding per token, the authors of the BERT paper tried different strategies: the first-layer embeddings, the embeddings from the second-to-last and last hidden layer, the weighted sum and the concatenation of the last four hidden layers and the weighted sum of all 12 layers.\n",
    "\n",
    "They obtain good results with the weighted sum and the concatenation of the last four hidden layers. Therefore, here we'll adopt the approach of **averaging the output of the last four hidden layer**. However, the best strategy is application dependent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3G9w2m1AsrJM"
   },
   "source": [
    "## Define strategies to combine the outputs from BERT\n",
    "\n",
    "As mentioned, we use the following strategies:\n",
    "\n",
    "1. We average the output of the last four hidden layer to obtain one embedding per token.\n",
    "2. We average all the token embeddings to obtain one embedding per sentence.\n",
    "\n",
    "There are other options available. For example, I'd recommend checking out [sentence-transformers](https://pypi.org/project/sentence-transformers/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwcabQhjRdEx"
   },
   "outputs": [],
   "source": [
    "def get_bert_embedding(tokens_id, model):\n",
    "    ''' Pass the tokens into the BERT model, then combines the embeddings of the last four \n",
    "    encoders into one final embedding for each non-special token.\n",
    "    The input is a torch tensor and a BERT model\n",
    "    the output is a numpy array in 2D '''\n",
    "    # model inference\n",
    "    with torch.no_grad():\n",
    "        bert_output = model(tokens_id.to(device), token_type_ids = None)\n",
    "\n",
    "    # keep the output embeddings of the last 4 encoders\n",
    "    last_hidden_states = [t.to('cpu').numpy() for t in bert_output[2][-4:]]\n",
    "\n",
    "    # concatenate and average them\n",
    "    token_embeddings = np.vstack(last_hidden_states).mean(axis = 0)\n",
    "\n",
    "    # remove special tokens [CLS] and [SEP]\n",
    "    N = token_embeddings.shape[0]\n",
    "    token_embeddings = token_embeddings[1:N-1]\n",
    "\n",
    "    return token_embeddings # should be of shape #tokens x 768\n",
    "\n",
    "def get_sentence_embedding(sentence, tokenizer, model):\n",
    "    ''' Returns the BERT based embedding of a sentence as the average across its\n",
    "    token embeddings\n",
    "    The output is a numpy array\n",
    "    (but check out sentence-transformers too: https://pypi.org/project/sentence-transformers/)\n",
    "    '''\n",
    "    feat_encoded_dict = tokenizer(sentence,\n",
    "                         add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                         return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                         padding = False\n",
    "                         )\n",
    "    token_embeddings = get_bert_embedding(feat_encoded_dict['input_ids'], model) #shape #tokens x 768\n",
    "    \n",
    "    # average across embeddings\n",
    "    sentence_embeddings = token_embeddings.mean(axis=0)\n",
    "\n",
    "    return sentence_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BVKijrgggVRr"
   },
   "outputs": [],
   "source": [
    "# process all skills: on the GPU it takes around 3 minutes, on the CPU likely more than an hour.\n",
    "# If using the CPU, I would suggest only processing the first 100 skills.\n",
    "t0 = time.time()\n",
    "skills_embeddings = []\n",
    "for i,skill in enumerate(esco_df.preferredLabel.to_list()[:101]):\n",
    "    skills_embeddings.append(get_sentence_embedding(skill, feat_tokenizer, bert_model))\n",
    "    if i % 1000 == 0 and i>0:\n",
    "      print(f'Time elapsed so far to embed {i} skills is {(time.time()- t0)/60:.2f} minutes')\n",
    "\n",
    "print(f'The size of each skills embeddings is {skills_embeddings[0].shape}')\n",
    "# concatenate them all\n",
    "skills_embeddings = np.vstack(skills_embeddings)\n",
    "\n",
    "print(f'The size of the skills embeddings is {skills_embeddings.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQ0lyxHKuKRp"
   },
   "source": [
    "COMPUTING SIMILARITY\n",
    "\n",
    "There are many similarity metrics out there. Howevert, the most commonly used one to check how similar two word embeddings are is the [\"cosine similarity\"](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html) or the \"dot product\" (if embeddings are normalized).\n",
    "\n",
    "In the following we will use the cosine similarity, which ranges between -1 and 1. -1 means word embeddings are opposite, 0 means there is no similarity, +1 means there is perfect similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jPk1wRl2gYsT"
   },
   "outputs": [],
   "source": [
    "# Potential applications. \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Let's say we have a new skill that might or might not be in the dataset already. \n",
    "# How do we make sense of it? One way is to select the existing skill that is the  \n",
    "# most similar to the 'new' one. This way we can understand how the new skill 'fits in'.\n",
    "new_skill = 'python programming language'\n",
    "\n",
    "new_skill_embedding = get_sentence_embedding(new_skill, feat_tokenizer, bert_model)\n",
    "\n",
    "# get the cosine similarity between the new skill and all the skills in the ESCO dataset\n",
    "# The cosine similarity measures how similar two vectors are through the angle between\n",
    "# the two vectors. The idea is that parallel vectors are the most similar (cosine\n",
    "# similarity = 1) and perpendicular vectors are the least similar (cosine similarity = 0)\n",
    "similarities = cosine_similarity(new_skill_embedding.reshape(1,-1), skills_embeddings)\n",
    "# currently is 2D, make it 1D\n",
    "similarities= similarities.flatten()\n",
    "\n",
    "# get the top 5 similar skills\n",
    "most_similar_skills_indices = list(np.argsort(similarities)[-5:])\n",
    "# reverse the order\n",
    "most_similar_skills_indices = most_similar_skills_indices[::-1]\n",
    "most_similar_skills = [esco_df.iloc[i].preferredLabel for i in most_similar_skills_indices]\n",
    "\n",
    "print(f'The 5 skills that are most similar to \"{new_skill}\" are:')\n",
    "for i,skill in zip(most_similar_skills_indices,most_similar_skills):\n",
    "  print(f\"'{skill}', with similarity {similarities[i]:.2f}\")\n",
    "\n",
    "# you can try it out with other skills that come to mind, or even more generic words.\n",
    "\n",
    "# We could also apply unsupervised algorithms to the dataset - for example, we can\n",
    "# group the skills into sets of similar items (a technique called clustering), like \n",
    "# all IT skills, all language skills, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DsJfKmSQtOkf"
   },
   "source": [
    "## Exercise\n",
    "\n",
    "1. Try computing the average token embedding in a different way. Instead of averaging the last 4 hidden layers, try using only the second-to-last hidden layer (this is another strategy used in the BERT paper). How do the results change?\n",
    "2. Try using a model other than BERT, how do the results change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WdK1VdK_t0cT"
   },
   "outputs": [],
   "source": [
    "#### Your code here\n",
    "\n",
    "# Solution\n",
    "# the function to aggregate the embedding can be modified like this:\n",
    "def get_bert_embedding(tokens_id, model):\n",
    "    ''' Pass the tokens into the BERT model, then combines the embeddings of the last four \n",
    "    encoders into one final embedding for each non-special token.\n",
    "    The input is a torch tensor and a BERT model\n",
    "    the output is a numpy array in 2D '''\n",
    "    # model inference\n",
    "    with torch.no_grad():\n",
    "        bert_output = model(tokens_id.to(device), token_type_ids = None)\n",
    "\n",
    "    # keep the output embeddings of the second to last hidden layer\n",
    "    second_last_hidden_state = [t.to('cpu').numpy() for t in bert_output[2][-2]]\n",
    "\n",
    "    # get the token embeddings as they are\n",
    "    token_embeddings = second_last_hidden_state[0]\n",
    "\n",
    "    # remove special tokens [CLS] and [SEP]\n",
    "    N = token_embeddings.shape[0]\n",
    "    token_embeddings = token_embeddings[1:N-1]\n",
    "\n",
    "    return token_embeddings # should be of shape #tokens x 768\n",
    "\n",
    "# An example of a different model is 'roberta-base' (others are here: https://huggingface.co/docs/transformers/main_classes/model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOywAZGJpmsy"
   },
   "source": [
    "# Build a question-answering system\n",
    "\n",
    "Please also refer to the original [HuggingFace tutorial](https://huggingface.co/course/chapter7/7?fw=pt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CEiVGeSYMnxS"
   },
   "source": [
    "## The SQUAD dataset\n",
    "\n",
    "From the [SQUAD website](https://rajpurkar.github.io/SQuAD-explorer/):\n",
    "\"Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\"\n",
    "\n",
    "Each data point in the dataset has the following features:\n",
    "1. The \"context\", that is the piece of text from which the answer is taken.\n",
    "2. A \"question\" to answer.\n",
    "3. One or more possible answers to the question. Each answer includes the text of the answer and where the answer starts in the context (the starting point is indicated as the position of the character at which the answer starts).\n",
    "\n",
    "SQUAD is an example of a dataset for **extractive** question-answering. This is when the answer is contained as-is within the context. You can also have a look at the [original paper](https://arxiv.org/pdf/1606.05250.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N9SXRcqqtzkZ"
   },
   "outputs": [],
   "source": [
    "# load the SQUAD dataset\n",
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3GMGUM_zMdJP"
   },
   "outputs": [],
   "source": [
    "# this will show the number of data points and their key features\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3RoMo45bGDL"
   },
   "source": [
    "### Some basic EDA\n",
    "\n",
    "(not an exhaustive collection of all that can be done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-cVWa-UjMuZB"
   },
   "outputs": [],
   "source": [
    "\n",
    "# What does an example data point looks like?\n",
    "print('\\n Example data point:')\n",
    "for k in raw_datasets['train'][1].keys():\n",
    "    print(k, ':', raw_datasets['train'][1][k])\n",
    "\n",
    "# For each question, how many possible answers are there?\n",
    "for split_name in ['train','validation']:\n",
    "    print(f\"\\n For the {split_name} split: \")\n",
    "    print(collections.Counter([len(t['text']) for t in raw_datasets[split_name]['answers']]))\n",
    "    #for lists, Counter is the equivalent of .value_counts() for dataframes\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bSC_UYpYhAZ"
   },
   "source": [
    "As you can see, all the training samples have only one possible answer, while the validation samples have multiple possible asnwers.\n",
    "\n",
    "This is because during training we can be a bit more prescriptive and \"force\" the algorithm towards one and one answer only. However, some questions may inherently have multiple valid answers (language can be ambiguous!) and we don't want to penalize our evaluation by excluding some of them. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uDyj5BP6Y_IO"
   },
   "outputs": [],
   "source": [
    "for t in ['context','question','answers']:\n",
    "    print(t.capitalize())\n",
    "    print(raw_datasets['validation'][2][t])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "COyBinfQXH-F"
   },
   "outputs": [],
   "source": [
    "# For the training dataset what's the length distribution for the answers (length as in number of words separated by a white space)?\n",
    "# We can image that longer answers are harder to detect, for example...\n",
    "# We restrict ourselves to the training dataset since we know there is always only one answer\n",
    "all_answers_length = [len(t['text'][0].split()) for t in raw_datasets['train']['answers']]\n",
    "plt.hist(all_answers_length, bins= 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fshv-xFkbdJ-"
   },
   "source": [
    "Most answers seem very short. It might be interesting to see how the algorithm performs with answers of different length..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0cnsInE4Psc"
   },
   "outputs": [],
   "source": [
    "# For the training dataset what's the length distribution for the context (length as in number of words separated by a white space)?\n",
    "all_contexts_length = [len(t.split()) for t in raw_datasets['train']['context']]\n",
    "plt.hist(all_contexts_length, bins= 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwHE6IdVbcqD"
   },
   "outputs": [],
   "source": [
    "# How about the position of the answers in the text? Is that equally distributed?\n",
    "# Let's get the relative position of each answer in the text (character starting position / number of characters )\n",
    "all_answers_starts = [t['answer_start'][0] for t in raw_datasets['train']['answers']]\n",
    "all_context_lengths = [len(t) for t in raw_datasets['train']['context']]\n",
    "relative_answers_starts = [n/d for n,d in zip(all_answers_starts,all_context_lengths)]\n",
    "plt.hist(relative_answers_starts, bins= 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08TUyc3fj4rU"
   },
   "source": [
    "As you can see, the distribution is not uniform. This is somewhat expected, probably for two reasons:\n",
    "1. Since the answer is a span of text, it can start at the beginning of the context, but it cannot start at the end, since otherwise it would have length zero.\n",
    "2. To create the dataset, the authors employed crowdworkers (paid 9$ per hours, BTW) to create up to 5 questions and answers for each context. Is it possible that the crowdworkers scanned the context left to right when creating the questions?\n",
    "\n",
    "Is it possible that part of what the model learns is to look for the answer at the beginning of the context even when it shouldn't?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6GjiNjXmpuI"
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLWNQO9XO2Qy"
   },
   "source": [
    "#### Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yw9Mnzf2P2RP"
   },
   "outputs": [],
   "source": [
    "# First, import the tokenizer\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5Zf9BHznMfJ"
   },
   "source": [
    "To solve a question-answering task, the input to our model is the concatenation of the question and the context (note the question goes first). The two are separated by the special character [SEP] so that the model knows where the context end and the questions starts. As usual, BERT will also add the special character [CLS] at the beginning of the input and the character [SEP] again at the end of the second sentence. Overall the input looks like this:\n",
    "\n",
    "`[CLS] question [SEP] context [SEP]`\n",
    "\n",
    "The required output is the indexes of the tokens that represent the beginning and the end of the answer.\n",
    "\n",
    "This means that extractive QA is equivalent to a token classification system: each token in the context is given two labels: one for being the start of the answer, one for being the end. Each label can be either 0 or 1, with 1 meaning that a token is the beginning or the end of the answer. What type of classification system so you think this is? (multi-class, multi-label, something else, ...)\n",
    "\n",
    "This image from the [HuggingFace tutorial](https://huggingface.co/course/chapter7/7?fw=pt#the-squad-dataset) might help clarify the setup:\n",
    "\n",
    "![Extractive QA labelling](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/qa_labels.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPZmXHuT4Im3"
   },
   "source": [
    "One problem is that some context might be very long. This is a problem because:\n",
    "1. The input to the transformer has to be a fixed length for the nextwork to be setup correctly.\n",
    "2. This is achieved by padding short texts (that is, adding filler tokens at the end) or by truncating long text (that, dropping all tokens after a maximum length).\n",
    "3. However, if we drop the tail of long texts, we risk losing the answer!\n",
    "\n",
    "To address this problem, we prepare the data by splitting long contexts into overlapping texts (windows) of the right size. For example, if we have a limit of 32 tokens (without considering question and special characters) and the following context:\n",
    "\n",
    "\n",
    "> Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\".\n",
    "\n",
    "We would split it into:\n",
    "\n",
    "> Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building\n",
    "\n",
    "> Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\".\n",
    "\n",
    "and use both windows as separate inputs to the neural network. For the windows that do not contain the right answer, the label would need to be set accordingly.\n",
    "\n",
    "The tokenizer function can help us thanks to the keyword `return_overflowing_tokens`, which tells the tokenizer we want to keep those tokens that otherwise would be lost due to the truncation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "RtOGZEDNPnze"
   },
   "outputs": [],
   "source": [
    "context = raw_datasets[\"train\"][0][\"context\"]\n",
    "question = raw_datasets[\"train\"][0][\"question\"]\n",
    "\n",
    "inputs = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    max_length=100,\n",
    "    truncation=\"only_second\", #only truncate the second sentence (the context) and not the first (the question)\n",
    "    stride=50, #sets how many overlapping tokens we want\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    ")\n",
    "\n",
    "for ids in inputs[\"input_ids\"]:\n",
    "    print(tokenizer.decode(ids))\n",
    "\n",
    "print(inputs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZEyQmcSLBKt"
   },
   "source": [
    "The output should be the different overlapping context windows, each preceded by the question (and including the special characters).\n",
    "\n",
    "The other thing we need to do is to correctly map the beginning of the answer to the right character in the context window (or windows) that actually contains the answer. In this case the answer is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gVfYYyq5L4Ci"
   },
   "outputs": [],
   "source": [
    "raw_datasets[\"train\"][0][\"answers\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtLvExijL-qW"
   },
   "source": [
    "We can see that the third and fourth context windows contain the answers, but the first and second don't. Furthermore, we need to find where in the windowed contexts is the answer. \n",
    "\n",
    "We can do this thanks to the fact that the tokenizer returns the `offset_mapping`: this keeps track of the position of each token in the original context. Given that we know where the answer starts (`raw_datasets[\"train\"][\"answers\"][\"answer_start\"]`) and how many characters long it is (`len(raw_datasets[\"train\"][\"answers\"][\"text\"])`) we can compute whether a given token is the start of the answer, the end of the answer or neither of them.\n",
    "\n",
    "The other thing to mention is that now we have many more data points. We need to keep track of which original data point the new shortened contexts came from. We can do this thanks to another piece of information returned by the tokenizer, the `overflow_to_sample_mapping`, that tells us exactly this.\n",
    "\n",
    "Putting it all together, we have the following preprocessing function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "42V1m1n2LKQd"
   },
   "outputs": [],
   "source": [
    "max_length = 384 #how long the input should be\n",
    "stride = 128 #how large should the overlap be between consecuting windows\n",
    "\n",
    "\n",
    "def preprocess_training_examples(examples):\n",
    "    \"\"\" split long contexts into overlapping windows and map to the right labels \"\"\"\n",
    "    questions = [q.strip() for q in examples[\"question\"]]  #.strip() is to remove extra white spaces at the beginning of some questions\n",
    "\n",
    "    # tokenize\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # get the map between the new data points to the original ones\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    # define the label for each new datapoint\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-D4i1fS6Oosi"
   },
   "source": [
    "Apply the pre-processing function to the training dataset using the `Dataset.map()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PKmR36ZQMtWm"
   },
   "outputs": [],
   "source": [
    "train_dataset = raw_datasets[\"train\"].map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")\n",
    "len(raw_datasets[\"train\"]), len(train_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8ewSMdRO5J2"
   },
   "source": [
    "#### Validation dataset\n",
    "\n",
    "Since the validation dataset is only used to get the performance metric, we only need to process all the contexts so that they fit within the maximum number of tokens.\n",
    "\n",
    "Indeed, the labels in the validation dataset are only needed to compute the performance metrics. As long as we can process the model output to be in the \"right shape\" we can pass the model output and the labels as they are to a HuggingFace function `metric = load_metric(\"squad\")` that will do the heavy lifting for us. Please see below for the needed post-processing.\n",
    "\n",
    "As a general tip, have a look at what metric functions HuggingFace (or other libraries) offer and **how** they expect to be given the predictions and the labels as input. This can save you a lot of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6uTk0nvZOuv7"
   },
   "outputs": [],
   "source": [
    "def preprocess_validation_examples(examples):\n",
    "    \"\"\" pre-process the question+context into chunks of appropriate length\"\"\"\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpeESIyXR851"
   },
   "outputs": [],
   "source": [
    "validation_dataset = raw_datasets[\"validation\"].map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
    ")\n",
    "len(raw_datasets[\"validation\"]), len(validation_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ejxQ0DcXVle3"
   },
   "outputs": [],
   "source": [
    "validation_dataset[3]['example_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtXtQs56V_U0"
   },
   "source": [
    "### Post-processing\n",
    "\n",
    "Remember that the output of the question answering model is a vector of scores for each token to determine whether any of them is the start or the end of the answer. To transform this input back into proper answers, we need to take the following steps:\n",
    "\n",
    "1. We mask the start and end logits corresponding to tokens outside of the context. The answer can only be in the context.\n",
    "2. We then convert the start and end logits (the scores) into probabilities using a softmax.\n",
    "3. We attribute a score to each (start_token, end_token) pair by taking the product of the corresponding two probabilities. This is because it's the whole answer span that we are after. This step is also necessary to allow for step 4.\n",
    "4. We look for the pair with the maximum score that yields a valid answer (e.g., a start_token lower than end_token).\n",
    "\n",
    "One problem with this approach is that there are many pairs to evaluate. Can we simplify? Yes, we can decide to only compute the joint pair probability (start_token, end_token) for the N best candidate start tokens and candidate end tokens. Furthermore, we can work directly on the logits and sum them, instead of multiplying the probabilities (remember that log(a*b) = log(a)+log(b) and that logit=log(probability)).\n",
    "\n",
    "Finally, the last thing we need to do is to to extract the span of text that goes from start_token to end_token and compare to the possible ground truth answers. Note that we do this for all the context windows derived from the original context. The overall predicted answer for each original data point is the answer with the highest score across all context windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ydJCLFngSuHJ"
   },
   "outputs": [],
   "source": [
    "# load the pre-defined squad metric from HuggingFace to make things easier\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"squad\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YyzYCs_4tdtH"
   },
   "source": [
    "The squad metric computes two numbers:\n",
    "\n",
    "1. The proportion of predicted answers that are exactly the same as (any of) the ground truth answers (exact).\n",
    "2. The average overlap between the predicted and ground truth answers as the average of the maximum F1 across all possible answers for a given question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmEBnw84SuDt"
   },
   "outputs": [],
   "source": [
    "# define function to compute the metric. The hard part is not the metric itself, \n",
    "# it's the post-processing of the output\n",
    "n_best = 20\n",
    "max_answer_length = 30\n",
    "\n",
    "def get_predicted_and_true_answers(start_logits, end_logits, features, examples):\n",
    "    \"\"\" \n",
    "    Get the best predicted answer and compare with ground truth\n",
    "    \"\"\"\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            # get the n_best candidates\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            # get the joint probability of (start_token, end_token) pairs\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    return {'predicted': predicted_answers, 'theoretical': theoretical_answers}\n",
    "\n",
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    out = get_predicted_and_true_answers(start_logits, end_logits, features, examples)\n",
    "    predicted_answers = out['predicted']\n",
    "    theoretical_answers = out['theoretical']\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLw4_c7_WCdq"
   },
   "source": [
    "### Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oYYtKhQrSuBd"
   },
   "outputs": [],
   "source": [
    "# load the question answering model (it's a good pre-trained one for this task). \n",
    "# Note that it's still based on bert-base-cased, so the tokenizer is the same\n",
    "model_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GwR2tMSWETL"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BKsI_EFjSt-n"
   },
   "outputs": [],
   "source": [
    "# set up the training hyperparameters (note the low learning rate since we're fine-tuning)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"bert-finetuned-squad\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    #fp16=True, #uncomment this if using the GPU\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZszrOFewSt0s"
   },
   "outputs": [],
   "source": [
    "# perform the actual training\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset.select(range(50)), # use  train_dataset.select(range(50)), if you want to speed things up\n",
    "    eval_dataset=validation_dataset.select(range(20)), # use  validation_dataset.select(range(20)), if you want to speed things up\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igWas8PxWINx"
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "Now that the training is done, we can check how well we can answer questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ScnhJJrZTfgS"
   },
   "outputs": [],
   "source": [
    "Nval=122 #use Nval=200 to speed things up when using the CPU\n",
    "predictions = trainer.predict(validation_dataset.select(range(Nval)))\n",
    "start_logits, end_logits = predictions.predictions\n",
    "compute_metrics(start_logits, end_logits, validation_dataset.select(range(Nval)), \n",
    "                raw_datasets[\"validation\"].select(range(Nval)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "paK9FU6CT7ma"
   },
   "outputs": [],
   "source": [
    "# let's show some examples\n",
    "n_examples= 3\n",
    "out = get_predicted_and_true_answers(start_logits, end_logits, validation_dataset.select(range(n_examples)), \n",
    "                                      raw_datasets[\"validation\"].select(range(n_examples)))\n",
    "for p, t in zip(out['predicted'],out['theoretical']):\n",
    "    print('Predicted answer:')\n",
    "    print(p['prediction_text']) \n",
    "    print('Desired answer(s):')\n",
    "    print(t['answers']['text'])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XxhbvlUwdKd"
   },
   "source": [
    "How does it look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IkqKAXS1tBMq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQtu7gcowPlR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "comp1804_tutorial_week12_NLP_apps_solutions.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
